---
title: "Homework assignment 2"
author: "David Utassy"
date: '2021 04 04 '
output: html_document
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Fashion MNIST data (10 points)

## Introduction
In this assignment, the task was to build deep neural net models to predict image classes. The goal is to have as accurate classifier as possible: we are using accuracy as a measure of predictive power. 

```{r includes, include=TRUE, results=F, warning=F, comment=F, message=F}
library(keras)
library(kableExtra)
```

## Data

In this assignment, I have to classify images from the “Fashion MNIST dataset” [see more here](https://github.com/zalandoresearch/fashion-mnist).

```{r get_data, include=TRUE, results=F, warning=F, comment=F, message=F}
fashion_mnist <- dataset_fashion_mnist()
x_train <- fashion_mnist$train$x
y_train <- fashion_mnist$train$y
x_test <- fashion_mnist$test$x
y_test <- fashion_mnist$test$y
```

### a, Show some example images from the data.

In the following code snippet, I created some helper functions to plot some images from the dataset. (some more detailed description can be seen in the comments)

```{r observe_data, include=TRUE, results=F, warning=F, comment=F, message=F}
rotate <- function(x) t(apply(x, 2, rev))

getLabel <- function(num){
  label <- switch(
   num+1,
   "T-shirt/top",
   "Trouser",
   "Pullover",
   "Dress",
   "Coat",
   "Sandal",
   "Shirt",
   "Sneaker",
   "Bag",
   "Ankle boot",
   "Unknown"
  )
  return(label)
}

showPic <- function(data, label, i) {
  image(
    rotate(data[i,,]),
    col = gray.colors(255), xlab=getLabel(if(label == 10) 10 else label[i] ), ylab = ""
  )
}

showFirstXPic <- function(X, data, label=10) {
  par(mfrow = c(X/3, 3))
  for (i in 1:X){
    showPic(data, label,i)
  }
}


showFirstXPic(6, x_train, y_train)
showFirstXPic(6, x_test)
```

## Building Neural Networks:

### b, Train a fully connected deep network to predict items.

#### Normalize the data similarly to what we saw with MNIST.

In the following code snippet, I am normalizing the data and transforming it into a form how keras needs it.

```{r data_prep_4_keras, include=TRUE, results=F, warning=F, comment=F, message=F}
#scale
x_train <- as.matrix(as.data.frame.array(x_train)) / 255
x_test <- as.matrix(as.data.frame.array(x_test)) / 255

#one-hot encodeing
y_train_label <- y_train
y_test_label <-y_test
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)

my_epoch <- 40

```

#### Experiment with network architectures and settings (number of hidden layers, number of nodes, activation functions, dropout, etc.). Explain what you have tried, what worked and what did not. Present a final model. Make sure that you use enough epochs so that the validation error starts flattening out - provide a plot about the training history (plot(history)). Evaluate the model on the test set. How does test error compare to validation error?

In the following code snippets, I have trained 5 different NN models. All the models have 784 input neurons as that is the number of pixels in each picture, and has 10 output neurons as that is the number of possible clothes that we want the model to classify. These output neurons have softmax activation function as we want to get probabilities for each type of clothes. 

### NN Model1

The first model has one hidden layer with 128 neurons and uses relu activation function. I also add a dropout “layer” here with a dropout ratio of 0.3.

```{r nn1, include=TRUE, results=F, warning=F, comment=F, message=F}
model1 <- keras_model_sequential()
model1 %>%
  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = 'softmax')

compile(
  model1,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

#model1_fit <- fit(
#  model1, x_train, y_train,
#  epochs = my_epoch, batch_size = 128,
#  validation_data = list(x_test, y_test),
#  verbose = 1,
#  callbacks = list(
#    callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1))
#)
model1 <- load_model_hdf5("models/model1.h5")

#saveRDS(model1_fit, "models/model1_fit.rds")
save_model_hdf5(model1, "models/model1.h5")
```

Note, that by training models for a high number of epochs, I realized on the validation_loss that models are starting to overfit after 15-25 epoch. I found a great solution in Keras for that, which is a callback function on the fit() function called ‘callback_reduce_lr_on_plateau()‘. It reduces the learning rate if the specified metric (val_loss) stops decreasing. By adding this function into my trainings my models flattened out in a great optimum instead of starting to jump out from the optimum because of the too high learning rate near optimal spots. (you can see the training history of the first model with this method below)

```{r plot_train, include=TRUE, results=F, warning=F, comment=F, message=F,  fig.align='center'}
model1_fit <- readRDS("models/model1_fit.rds")
plot(model1_fit, smooth = F, theme_bw = getOption("keras.plot.history.theme_bw", FALSE))
```

### NN Model 2

In the second model, I tried adding one more hidden layer with 128 neurons and relu again. This turned out to be a bad idea according to the validation accuracy.

```{r nn2, include=TRUE, results=F, warning=F, comment=F, message=F}
model2 <- keras_model_sequential()
model2 %>%
  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = 'softmax')

compile(
  model2,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

#model2_fit <-  fit(
#  model2, x_train, y_train,
#  epochs = my_epoch, batch_size = 128,
#  validation_data = list(x_test, y_test), 
#  callbacks = list(
#    callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1))
#)
model2 <- load_model_hdf5("models/model2.h5")

#saveRDS(model2_fit, "models/model2_fit.rds")
save_model_hdf5(model2, "models/model2.h5")
```

### NN Model 3

In the third model according to my previous experience, I went back using one hidden layer and changed the dropout ratio to 0.2. It turned out to be a good decision as validation accuracy increased.

```{r nn3, include=TRUE, results=F, warning=F, comment=F, message=F}
model3 <- keras_model_sequential()
model3 %>%
  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 10, activation = 'softmax')

compile(
  model3,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

#model3_fit <-  fit(
#  model3, x_train, y_train,
#  epochs = my_epoch, batch_size = 128,
#  validation_data = list(x_test, y_test), 
#  callbacks = list(
#    callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1))
#)
model3 <- load_model_hdf5("models/model3.h5")

#saveRDS(model3_fit, "models/model3_fit.rds")
save_model_hdf5(model3, "models/model3.h5")
```

### NN Model 4

In the fourth model, I sticked with the 0.2 dropout rate and tried to change the number of neurons in the first hidden layer to 64, which decreased accuracy slightly.

```{r nn4, include=TRUE, results=F, warning=F, comment=F, message=F}
model4 <- keras_model_sequential()
model4 %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(784)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 10, activation = 'softmax')

compile(
  model4,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

#model4_fit <-  fit(
#  model4, x_train, y_train,
#  epochs = my_epoch, batch_size = 128,
#  validation_data = list(x_test, y_test), 
#  callbacks = list(
#    callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1))
#)
model4 <- load_model_hdf5("models/model4.h5")

#saveRDS(model4_fit, "models/model4_fit.rds")
save_model_hdf5(model4, "models/model4.h5")
```

### NN Model 5

In the fifth model, I tried adding one more hidden layer again but in contrast to model 2 I tried with 64 neurons now. This turned out to be a good idea as it increased accuracy.
```{r nn5, include=TRUE, results=F, warning=F, comment=F, message=F}
model5 <- keras_model_sequential()
model5 %>%
  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 10, activation = 'softmax')

compile(
  model5,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

#model5_fit <-  fit(
#  model5, x_train, y_train,
#  epochs = my_epoch, batch_size = 128,
#  validation_data = list(x_test, y_test), 
#  callbacks = list(
#    callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1))
#)
model5 <- load_model_hdf5("models/model5.h5")

#saveRDS(model5_fit, "models/model5_fit.rds")
save_model_hdf5(model5, "models/model5.h5")
```

In the table below you can see the statistics of each neural network, I described above.

```{r res1, echo = FALSE , results = "asis", warning = FALSE, message = FALSE, cache=F }
results_nn <- data.frame(stat = c('val_loss', 'val_accuracy')) 
my_nn_models <- c(model1, model2, model3, model4, model5) 
for(model in my_nn_models){
  results_nn <- cbind(results_nn, evaluate(model, x_test, y_test))
}

colnames(results_nn) <- c('stat', 'nn_model1', 'nn_model2', 'nn_model3', 'nn_model4', 'nn_model5')
results_nn$stat <- NULL
results_nn <- t(results_nn)
colnames(results_nn) <- c('loss', 'accuracy')
kbl(results_nn, digits = 3)  %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
```

## Building Convolutional Neural Networks:

#### Try building a convolutional neural network and see if you can improve test set performance. Just like before, experiment with different network architectures, regularization techniques and present your findings

The following code snippet is needed to reshape data for CNNs.
```{r reshape-for-conv, include=T, results=F, warning=F, comment=F, message=F}
x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))
```

I have trained 5 different CNN in order to reach the best model. Note, that I also used the adaptive learning rate option of Keras here as described before NNs.

All the models have an input shape of 28x28x1 as the pictures are in that shape, and have 10 output neurons as that is the number of possible clothes that we want the model to classify. These output neurons have softmax activation function as we want to get probabilities for each type of clothes.

### CNN Model 1

In the first model, I used a 2D convolutional layer with 32 filters (of 3x3) and with a relu activation function. Then, I added a 2D pooling layer of size 2x2 with an additional dropout of 0.25 ratio. After that came a simple flattening layer and the last hidden layer which was a dense layer of 16 neurons with relu. 

```{r cnn1, include=TRUE, results=F, warning=F, comment=F, message=F}
cnn_model1 <- keras_model_sequential()
cnn_model1 %>%
  layer_conv_2d(
    filters = 32,
    kernel_size = c(3, 3),
    activation = 'relu',
    input_shape = c(28, 28, 1)
  ) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

compile(
  cnn_model1,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

#cnn_model1_fit <-  fit(
#  cnn_model1, x_train, y_train,
#  epochs = my_epoch, batch_size = 128,
#  validation_data = list(x_test, y_test), 
#  callbacks = list(
#    callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1))
#)
cnn_model1 <- load_model_hdf5("models/cnn_model1.h5")

#saveRDS(cnn_model1_fit, "models/cnn_model1_fit.rds")
save_model_hdf5(cnn_model1, "models/cnn_model1.h5")
```

### CNN Model 2 and 3

The second and the third model is based on the first model, all I changed was the dropout rate (to 0.2 and 0.3). As the 0.2 dropout rate turned out to be better I sticked with that one in the following model.

```{r cnn2, include=TRUE, results=F, warning=F, comment=F, message=F}
cnn_model2 <- keras_model_sequential()
cnn_model2 %>%
  layer_conv_2d(
    filters = 32,
    kernel_size = c(3, 3),
    activation = 'relu',
    input_shape = c(28, 28, 1)
  ) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_flatten() %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

compile(
  cnn_model2,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

#cnn_model2_fit <-  fit(
#  cnn_model2, x_train, y_train,
#  epochs = my_epoch, batch_size = 128,
#  validation_data = list(x_test, y_test), 
#  callbacks = list(
#    callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1))
#)
cnn_model2 <- load_model_hdf5("models/cnn_model2.h5")

#saveRDS(cnn_model2_fit, "models/cnn_model2_fit.rds")
save_model_hdf5(cnn_model2, "models/cnn_model2.h5")
```
```{r cnn3, include=TRUE, results=F, warning=F, comment=F, message=F}
cnn_model3 <- keras_model_sequential()
cnn_model3 %>%
  layer_conv_2d(
    filters = 32,
    kernel_size = c(3, 3),
    activation = 'relu',
    input_shape = c(28, 28, 1)
  ) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.3) %>%
  layer_flatten() %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

compile(
  cnn_model3,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

#cnn_model3_fit <-  fit(
#  cnn_model3, x_train, y_train,
#  epochs = my_epoch, batch_size = 128,
#  validation_data = list(x_test, y_test), 
#  callbacks = list(
#    callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1))
#)
cnn_model3 <- load_model_hdf5("models/cnn_model3.h5")

#saveRDS(cnn_model3_fit, "models/cnn_model3_fit.rds")
save_model_hdf5(cnn_model3, "models/cnn_model3.h5")
```

### CNN Model 4

In the fourth model, I used the 0.2 dropout rate and I also changed the number of neurons in the last hidden dense layer from 16 to 128. This turned out to be a good idea as my validation accuracy increased by almost 1 percent.
```{r cnn4, include=TRUE, results=F, warning=F, comment=F, message=F}
cnn_model4 <- keras_model_sequential()
cnn_model4 %>%
  layer_conv_2d(
    filters = 32,
    kernel_size = c(3, 3),
    activation = 'relu',
    input_shape = c(28, 28, 1)
  ) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_flatten() %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

compile(
  cnn_model4,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

#cnn_model4_fit <-  fit(
#  cnn_model4, x_train, y_train,
#  epochs = my_epoch, batch_size = 128,
#  validation_data = list(x_test, y_test), 
#  callbacks = list(
#    callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1, patience = 5))
#)
cnn_model4 <- load_model_hdf5("models/cnn_model4.h5")

#saveRDS(cnn_model4_fit, "models/cnn_model4_fit.rds")
save_model_hdf5(cnn_model4, "models/cnn_model4.h5")
```

### CNN Model 5

In the fifth model I decided to make some google search to find a more sophisticated model [under this link](https://towardsdatascience.com/the-4-convolutional-neural-network-models-that-can-classify-your-fashion-images-9fe7f3e5399d) I found the following model:
```{r cnn5, include=TRUE, results=F, warning=F, comment=F, message=F}
cnn_model5 <- keras_model_sequential()
cnn_model5 %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = 'relu', input_shape = c(28, 28, 1)) %>%
  layer_batch_normalization() %>% 

  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = 'relu') %>%
  layer_batch_normalization() %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.25) %>%
  
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = 'relu') %>%
  layer_batch_normalization() %>% 
  layer_dropout(rate = 0.25) %>%
  
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = 'relu') %>%
  layer_batch_normalization() %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.25) %>%
  
  layer_flatten() %>%
  layer_dense(units = 512, activation = 'relu') %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.5) %>%
  
  layer_dense(units = 10, activation = 'softmax')

compile(
  cnn_model5,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

#cnn_model5_fit <-  fit(
#  cnn_model5, x_train, y_train,
#  epochs = 20, batch_size = 128,
#  validation_data = list(x_test, y_test), 
#  callbacks = list(
#    callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1, patience = 5, min_delta = 1e-03))
#)
cnn_model5 <- load_model_hdf5("models/cnn_model5.h5")

#saveRDS(cnn_model5_fit, "models/cnn_model5_fit.rds")
save_model_hdf5(cnn_model5, "models/cnn_model5.h5")
```

It’s performance was great but according to validation accuracy, it can not beat model 4. Also, I would like to note here that I only trained for 20 epochs as the training of this model was very time-consuming. Training further might have improved its results.
 
In the table below you can see the statistics of each neural network, I described above.
```{r res2, echo = FALSE , results = "asis", warning = FALSE, message = FALSE, cache=F }
results_cnn <- data.frame(stat = c('val_loss', 'val_accuracy')) 
my_cnn_models <- c(cnn_model1, cnn_model2, cnn_model3, cnn_model4, cnn_model5)
for(model in my_cnn_models){
  results_cnn <- cbind(results_cnn, evaluate(model, x_test, y_test))
}

colnames(results_cnn) <- c('stat', 'cnn_model1', 'cnn_model2', 'cnn_model3', 'cnn_model4', 'cnn_model5')
results_cnn$stat <- NULL
results_cnn <- t(results_cnn)
colnames(results_cnn) <- c('loss', 'accuracy')

kbl(results_cnn, digits = 3)  %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
```

# Summary

As you can see from the accuracy results CNN-s perform better than normal Deep Neural networks on image processing, however, they are pretty accurate as well. 

# 2. Hot dog or not hot dog? (12 points) OPTIONAL

In this problem you are going to predict if a certain image containing food is hot dog or is something else. 

```{r includes2, include=TRUE, results=F, warning=F, comment=F, message=F}
library(keras)
library(here)
library(grid)
library(magick)
library(filesstrings)
library(kableExtra)
```

#### a, Pre-process data so that it is acceptable by Keras (set folder structure, bring images to the same size, etc).
```{r prep_data2, include=TRUE, results=F, warning=F, comment=F, message=F}
# Setting parameters for image data generators

train_datagen <-  image_data_generator(rescale = 1/255)
validation_datagen <- image_data_generator(rescale = 1/255)  
test_datagen <-image_data_generator(rescale = 1/255)  


#Generating data

image_size <- c(150, 150)
batch_size <- 50

train_generator <- flow_images_from_directory(
  file.path(here(), "data/hot-dog-not-hot-dog/train/"),
  train_datagen,              # Data generator
  target_size = image_size,  # Resizes all images to 150 × 150
  batch_size = batch_size,
  class_mode = "binary"       # binary_crossentropy loss for binary labels
)

validation_generator <- flow_images_from_directory(
  file.path(here(), "data/hot-dog-not-hot-dog/validation/"),   
  validation_datagen,
  target_size = image_size,
  batch_size = batch_size,
  class_mode = "binary"
)

test_generator <- flow_images_from_directory(
  file.path(here(), "data/hot-dog-not-hot-dog/test/"),
  test_datagen,
  target_size = image_size,
  batch_size = batch_size,
  class_mode = "binary"
)
```

#### b, Estimate a convolutional neural network to predict if an image contains a hot dog or not. Evaluate your model on the test set.

In the code snippet below, you can see two different CNN models. The first one was a base model, while the second one is the result of a lot of experimentation. With this dataset, these models performed poorly as you can see in the summary table at the end of this exercise.  

```{r b2, include=TRUE, results=F, warning=F, comment=F, message=F}
#Define model1
hot_dog_model1 <- keras_model_sequential() 
hot_dog_model1 %>% 
  layer_conv_2d(filters = 32,
                kernel_size = c(3, 3), 
                activation = 'relu',
                input_shape = c(150, 150, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 16,
                kernel_size = c(3, 3), 
                activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 16,
                kernel_size = c(3, 3), 
                activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.25) %>% 
  layer_flatten() %>% 
  layer_dense(units = 8, activation = 'relu') %>% 
  layer_dense(units = 1, activation = "sigmoid")   # for binary

hot_dog_model1 %>% compile(
  loss = "binary_crossentropy",
  optimizer = "rmsprop",
  metrics = c("accuracy")
)

#hotdog_model1_fit <- hot_dog_model1 %>% fit(
#  train_generator,
#  steps_per_epoch = train_generator$n / batch_size,
#  epochs = 20,
#  validation_data = validation_generator,
#  validation_steps = validation_generator$n / batch_size, 
#  callbacks = list(
#    callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1))
#)
#saveRDS(hotdog_model1_fit, "models/hotdog_model1_fit.rds")
#save_model_hdf5(hot_dog_model1, "models/hot_dog_model1.h5")
#evaluate(hot_dog_model1, test_generator)

hot_dog_model1 <- load_model_hdf5("models/hot_dog_model1.h5")

#Define model2
#https://github.com/matiascaputti/not-hotdog/blob/master/1-%20Not%20Hotdog%20-%20base.ipynb

hot_dog_model2 <- keras_model_sequential() 
hot_dog_model2 %>% 
  layer_conv_2d(filters = 64,
                kernel_size = c(2, 2), 
                activation = 'relu',
                input_shape = c(150, 150, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  
  layer_conv_2d(filters = 32,
                kernel_size = c(2, 2), 
                activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.2) %>%
  
  layer_global_average_pooling_2d() %>% # this converts our 3D feature maps to 1D feature vectors
  layer_dense(units = 1024, activation = 'relu') %>% 
  layer_dropout(rate = 0.4) %>%

  layer_dense(units = 1, activation = "sigmoid")   # for binary

hot_dog_model2 %>% compile(
  loss = "binary_crossentropy",
  optimizer = 'rmsprop',
  metrics = c("accuracy")
)

#hotdog_model2_fit <- hot_dog_model2 %>% fit(
#  train_generator,
#  steps_per_epoch = train_generator$n / batch_size,
#  epochs = 40,
#  validation_data = validation_generator,
#  validation_steps = validation_generator$n / batch_size,
#  callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1)
#)
#saveRDS(hotdog_model2_fit, "models/hotdog_model2_fit.rds")
#save_model_hdf5(hot_dog_model2, "models/hot_dog_model2.h5")
#evaluate(hot_dog_model2, test_generator)

hot_dog_model2 <- load_model_hdf5("models/hot_dog_model2.h5")
```

#### c, Could data augmentation techniques help with achieving higher predictive accuracy? Try some augmentations that you think make sense and compare

I have tried 3 different augmentations with the best model I had from the previous section in the following code snippet. At the end of this exercise, you can see that some improved the validation accuracy, but I also checked the accuracy on the test sample which showed different results (no improvement compared to the best model in the previous chapter).

```{r c2, include=TRUE, results=F, warning=F, comment=F, message=F}
#DATA AUG1
train_datagen = image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)
train_generator <- flow_images_from_directory(
  file.path(here(), "/data/hot-dog-not-hot-dog/train"), # Target directory  
  train_datagen,              # Data generator
  target_size = image_size,  # Resizes all images to 150 × 150
  batch_size = batch_size,
  class_mode = "binary"       # binary_crossentropy loss for binary labels
)

#hotdog_model2_fit_aug1 <- hot_dog_model2 %>% fit(
#  train_generator,
#  steps_per_epoch = train_generator$n / batch_size,
#  epochs = 20,
#  validation_data = validation_generator,
#  validation_steps = validation_generator$n / batch_size,
#  callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1)
#)
#saveRDS(hotdog_model2_fit_aug1, "models/hotdog_model2_fit_aug1.rds")
#save_model_hdf5(hot_dog_model2, "models/hot_dog_model2_aug1.h5")
#evaluate(hot_dog_model2, test_generator)

hot_dog_model2_aug1 <- load_model_hdf5("models/hot_dog_model2_aug1.h5")

#DATA AUG2
train_datagen =  image_data_generator(
  rescale = 1/255,
  rotation_range = 180,
  width_shift_range = 0.4,
  height_shift_range = 0.4,
  shear_range = 0.4,
  zoom_range = 0.4,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)
train_generator <- flow_images_from_directory(
  file.path(here(), "/data/hot-dog-not-hot-dog/train"), # Target directory  
  train_datagen,              # Data generator
  target_size = image_size,  # Resizes all images to 150 × 150
  batch_size = batch_size,
  class_mode = "binary"       # binary_crossentropy loss for binary labels
)

#hotdog_model2_fit_aug1 <- hot_dog_model2 %>% fit(
#  train_generator,
#  steps_per_epoch = train_generator$n / batch_size,
#  epochs = 20,
#  validation_data = validation_generator,
#  validation_steps = validation_generator$n / batch_size,
#  callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1)
#)
#saveRDS(hotdog_model2_fit_aug2, "models/hotdog_model2_fit_aug2.rds")
#save_model_hdf5(hot_dog_model2, "models/hot_dog_model2_aug2.h5")
#evaluate(hot_dog_model2, test_generator)

hot_dog_model2_aug2 <- load_model_hdf5("models/hot_dog_model2_aug2.h5")

#DATA AUG3
train_datagen = image_data_generator(
  rescale = 1/255,
  rotation_range = 90,
  width_shift_range = 0.3,
  height_shift_range = 0.3,
  shear_range = 0.3,
  zoom_range = 0.3,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)
train_generator <- flow_images_from_directory(
  file.path(here(), "/data/hot-dog-not-hot-dog/train"), # Target directory  
  train_datagen,              # Data generator
  target_size = image_size,  # Resizes all images to 150 × 150
  batch_size = batch_size,
  class_mode = "binary"       # binary_crossentropy loss for binary labels
)

#hotdog_model2_fit_aug3 <- hot_dog_model2 %>% fit(
#  train_generator,
#  steps_per_epoch = train_generator$n / batch_size,
#  epochs = 20,
#  validation_data = validation_generator,
#  validation_steps = validation_generator$n / batch_size,
#  callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1)
#)
#saveRDS(hotdog_model2_fit_aug3, "models/hotdog_model2_fit_aug3.rds")
#save_model_hdf5(hot_dog_model2, "models/hot_dog_model2_aug3.h5")
#evaluate(hot_dog_model2, test_generator)

hot_dog_model2_aug3 <- load_model_hdf5("models/hot_dog_model2_aug3.h5")

model_eval_list <- list()
model_eval_list['final_model_no_aug'] <- as.data.frame(evaluate(hot_dog_model2, validation_generator))
model_eval_list['final_model_aug1'] <- as.data.frame(evaluate(hot_dog_model2_aug1, validation_generator))
model_eval_list['final_model_aug2'] <- as.data.frame(evaluate(hot_dog_model2_aug2, validation_generator))
model_eval_list['final_model_aug3'] <- as.data.frame(evaluate(hot_dog_model2_aug3, validation_generator))

model_eval_list_test <- list()
model_eval_list_test['final_model_no_aug'] <- as.data.frame(evaluate(hot_dog_model2, test_generator))
model_eval_list_test['final_model_aug1'] <- as.data.frame(evaluate(hot_dog_model2_aug1, test_generator))
model_eval_list_test['final_model_aug2'] <- as.data.frame(evaluate(hot_dog_model2_aug2, test_generator))
model_eval_list_test['final_model_aug3'] <- as.data.frame(evaluate(hot_dog_model2_aug3, test_generator))
```

#### d, Try to rely on some pre-built neural networks to aid prediction. Can you achieve a better performance using transfer learning for this problem?

In the following code snippet, I relied on two different pre-built NN to aid my prediction. As you can see in the summary table at the end they performed significantly better than previous models. 
```{r d2, include=TRUE, results=F, warning=F, comment=F, message=F}
# PRE TRAINED MODELS
 
#N 1
# imagenetet w INCEPLTION_V3
conv_base <- application_inception_v3(
  weights = "imagenet",
  include_top = FALSE,
  input_shape = c(image_size, 3)
)

#summary(conv_base)
freeze_weights(conv_base)

hot_dog_model2_inception_v3 <- keras_model_sequential()%>%
  conv_base %>% 
  layer_global_average_pooling_2d() %>% 
  layer_dense(units = 16, activation = 'relu') %>% 
  layer_dense(units = 1, activation = 'sigmoid')

hot_dog_model2_inception_v3 %>% compile(
  loss = "binary_crossentropy",
  optimizer = 'rmsprop',
  metrics = c("accuracy")
)

#hot_dog_model2_inception_v3_fit <- hot_dog_model2_inception_v3 %>% fit(
#  train_generator,
#  steps_per_epoch = train_generator$n / batch_size, 
#  epochs = 15,
#  validation_data = validation_generator,
#  validation_steps = validation_generator$n / batch_size, 
#  callbacks = list(
#   callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1, patience = 3))
#)
#saveRDS(hot_dog_model2_inception_v3_fit, "models/hot_dog_model2_inception_v3_fit.rds")
#save_model_hdf5(hot_dog_model2_inception_v3, "models/hot_dog_model2_inception_v3.h5")
#evaluate(hot_dog_model2_inception_v3, test_generator)

hot_dog_model2_inception_v3 <- load_model_hdf5("models/hot_dog_model2_inception_v3.h5")

model_eval_list['final_imagenet_inception_v3'] <- as.data.frame(evaluate(hot_dog_model2_inception_v3, validation_generator))
model_eval_list_test['final_imagenet_inception_v3'] <- as.data.frame(evaluate(hot_dog_model2_inception_v3, test_generator))


#N 2
# imagenetet w MOBILENET
base_model <- application_mobilenet(
  weights = 'imagenet', 
  include_top = FALSE,
  input_shape = c(image_size, 3))

#summary(base_model)
freeze_weights(base_model)

hot_dog_model2_mobilenet <- keras_model_sequential()%>%
  base_model %>% 
  layer_global_average_pooling_2d() %>% 
  layer_dense(units = 16, activation = 'relu') %>% 
  layer_dense(units = 1, activation = 'sigmoid')

hot_dog_model2_mobilenet %>% compile(
  loss = "binary_crossentropy",
  optimizer = 'rmsprop',
  metrics = c("accuracy")
)

#hot_dog_model2_mobilenet_fit <- hot_dog_model2_mobilenet %>% fit(
# train_generator,
# steps_per_epoch = train_generator$n / batch_size,
# epochs = 10,
# validation_data = validation_generator,
# validation_steps = validation_generator$n / batch_size, 
# callbacks = list(
#   callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1, patience = 5))
#)
#saveRDS(hot_dog_model2_mobilenet_fit, "models/hot_dog_model2_mobilenet_fit.rds")
#save_model_hdf5(hot_dog_model2_mobilenet, "models/hot_dog_model2_mobilenet.h5")
#evaluate(hot_dog_model2_mobilenet, test_generator)

hot_dog_model2_mobilenet <- load_model_hdf5("models/hot_dog_model2_mobilenet.h5")

model_eval_list['final_imagenet_mobilenet'] <- as.data.frame(evaluate(hot_dog_model2_mobilenet, validation_generator))
model_eval_list_test['final_imagenet_mobilenet_test'] <- as.data.frame(evaluate(hot_dog_model2_mobilenet, test_generator))

res_valid <- t(as.data.frame(model_eval_list))
res_test <- t(as.data.frame(model_eval_list_test))


colnames(res_valid) <- c('validation loss','validation accuracy')
colnames(res_test) <- c('validation loss','validation accuracy')
```

 
In the following table, you can see the validation loss and validation accuracy statistics of all the models trained above. You can see that both models based on other pre-trained models performed significantly better than others. I also evaluated the best model on the test set which resulted in 0.818 accuracy meaning that the model is kind of robust as it is just slightly lower than the validation accuracy.


```{r res2d, echo = FALSE , results = "asis", warning = FALSE, message = FALSE, cache=F }
knitr::kable(res_valid, caption = "", digits = 3 ) %>% 
  kable_styling( position = "center", latex_options = 'hold_position', bootstrap_options = c("striped", "hover"))
```
